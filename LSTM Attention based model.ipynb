{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import pdb\n",
    "import math\n",
    "import numpy\n",
    "import torch \n",
    "\n",
    "#Returns sequences of shuffled sentence and label\n",
    "class SequenceBatcher(object):\n",
    "  def __init__(self, device):\n",
    "    self.device = device\n",
    "    return\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    x, y = zip(*batch)\n",
    "    max_x_len = max(len(xx) for xx in x)\n",
    "    \n",
    "    x = torch.LongTensor([xx + [0] * (max_x_len - len(xx)) for xx in x])\n",
    "    y = torch.FloatTensor([yy for yy in y])\n",
    "\n",
    "    return x.to(self.device),y.to(self.device)\n",
    "\n",
    "#Shuffles and returns dataset\n",
    "class Shuffler(torch.utils.data.Dataset):\n",
    "  def __init__(self, corpus, vocabs, neg_size, corpus_size, max_snt=0, unk='<unknown>', neg_rate=0.1, weight=True):\n",
    "    self.corpus = corpus\n",
    "    self.neg_size = neg_size\n",
    "    self.neg_rate = neg_rate\n",
    "\n",
    "    self.weight = weight\n",
    "\n",
    "    num_grammatical_sentences = max_snt if (max_snt > 0) else corpus_size\n",
    "    self.shuffle_index = [0] + [int(random.random() < self.neg_rate) * random.randrange(self.neg_size) for _ in range(1,num_grammatical_sentences)]\n",
    "    self.temp_index = np.copy(self.shuffle_index)\n",
    "    self.num_items = sum(self.shuffle_index) + sum([int(i==0) for i in self.shuffle_index])\n",
    "\n",
    "    self.vocabs = vocabs\n",
    "    self.unk = unk\n",
    "\n",
    "    self.corpus_fp = open(corpus, mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "    self.grammatical_snt_idx = 0\n",
    "\n",
    "    self.x = [] # contains next grammatical sequence\n",
    "    self.tmp=[]\n",
    "    self.j=0\n",
    "    g=self.create_dataset()\n",
    "    self.A=g[0]\n",
    "    self.label=g[1]\n",
    "    return\n",
    "\n",
    "  def readline(self):\n",
    "    return self.corpus_fp.readline().lower().split()\n",
    "\n",
    "  def tok2id(self, tokens):\n",
    "    for ii, token in enumerate(tokens):\n",
    "      try:\n",
    "        token_id = self.vocabs[token]\n",
    "      except KeyError:\n",
    "        token_id = self.vocabs[self.unk]\n",
    "      tokens[ii] = token_id\n",
    "    return tokens\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.num_items\n",
    "\n",
    "  def create_dataset(self):\n",
    "        A=[]\n",
    "        y=[]\n",
    "        for i in range(self.num_items):\n",
    "            if not self.temp_index[self.grammatical_snt_idx]:\n",
    "                A.append(self.tok2id(self.readline()))\n",
    "                y.append(1)\n",
    "                self.grammatical_snt_idx += 1\n",
    "            else:\n",
    "                if self.temp_index[self.grammatical_snt_idx]==self.shuffle_index[self.grammatical_snt_idx]:\n",
    "                    self.tmp=self.tok2id(self.readline())\n",
    "                self.temp_index[self.grammatical_snt_idx] -= 1\n",
    "                A.append(random.sample(self.tmp[:-1],len(self.tmp[:-1])))\n",
    "                y.append(0)\n",
    "                if self.temp_index[self.grammatical_snt_idx]==0:\n",
    "                    self.grammatical_snt_idx += 1\n",
    "                    \n",
    "                \n",
    "        return [A,y]\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "        return self.A[index],self.label[index]\n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import utils\n",
    "import os\n",
    "import lzma # to read xz files\n",
    "\n",
    "import pickle\n",
    "\n",
    "class WordVector :\n",
    "  def __init__(self) :\n",
    "    return\n",
    "\n",
    "  def __init__(self, path, unk=\"<unk_vocab>\", beg='<s>', end='</s>', vcb_list=None) :\n",
    "    self.path = path \n",
    "    self.dim = -1\n",
    "    self.unk = unk\n",
    "    self.beg = beg\n",
    "    self.end = end\n",
    "    self.delim = \" \"\n",
    "    self.dic_pickle_path = self.path + '.dic.pkl'\n",
    "    self.mat_pickle_path = self.path + '.mat.pkl'\n",
    "    self.dictionary = {}\n",
    "    self.batch_sz = 100000\n",
    "    self.mat = np.zeros(shape=(0,0))\n",
    "\n",
    "    try :\n",
    "      with open(self.dic_pickle_path, 'rb') as dic_handle, open(self.mat_pickle_path, 'rb') as mat_handle:\n",
    "        print((\"Loading from the existing pickle file {0}\".format(path + '.{dic,mat}.pkl')))\n",
    "        self.dictionary = pickle.load(dic_handle)\n",
    "        self.mat = pickle.load(mat_handle)\n",
    "        self.dim = self.mat.shape[1]\n",
    "    except FileNotFoundError: \n",
    "      self.size = os.stat(path).st_size \n",
    "      print(\"HELOOOOO\")\n",
    "      self.load() \n",
    "\n",
    "    if not (self.unk in list(self.dictionary.keys())) : \n",
    "      print((\"Warning: could not find the unknown token {0}. A zero vector will be used instead\".format(self.unk)))\n",
    "      self.update_item(self.unk, np.array([0]*self.dim)) \n",
    "    if not (self.beg in list(self.dictionary.keys())) : \n",
    "      print((\"Warning: could not find the sentence beginning token {0}. A zero vector will be used instead\".format(self.beg)))\n",
    "      self.update_item(self.beg, np.array([0]*self.dim)) \n",
    "    if not (self.end in list(self.dictionary.keys())) : \n",
    "      print((\"Warning: could not find the sentence ending token {0}. A zero vector will be used instead\".format(self.end)))\n",
    "      self.update_item(self.end, np.array([0]*self.dim)) \n",
    "\n",
    "    self.vocabs = list(self.dictionary.keys())\n",
    "\n",
    "    if not os.path.isfile(self.dic_pickle_path):\n",
    "      with open(self.dic_pickle_path, 'wb') as handle:\n",
    "        pickle.dump(self.dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    if not os.path.isfile(self.mat_pickle_path):\n",
    "      with open(self.mat_pickle_path, 'wb') as handle:\n",
    "        pickle.dump(self.mat, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    if vcb_list is not None:\n",
    "      vcb_intersect = set(self.vocabs).intersection(set(vcb_list))\n",
    "      vcb_intersect = vcb_intersect.union(set([self.unk, self.beg, self.end]))\n",
    "      mat = np.zeros((len(vcb_intersect), self.dim))\n",
    "      dic = {}\n",
    "      for i,w in enumerate(list(vcb_intersect)):\n",
    "        dic[w] = i\n",
    "        mat[i] = self[w]\n",
    "      self.dictionary = dic\n",
    "      self.mat = mat\n",
    "      self.vocabs = list(vcb_intersect)\n",
    "\n",
    "\n",
    "\n",
    "  def __getitem__(self, w): \n",
    "    if isinstance(w,int) :\n",
    "      return self.mat[w]\n",
    "\n",
    "    try :\n",
    "      return self.mat[self.dictionary[w]]\n",
    "    except KeyError :\n",
    "      return self.mat[self.dictionary[self.unk]]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dictionary)\n",
    "\n",
    "  def __setitem__(self, w, val): \n",
    "    assert(len(val) == self.dim)\n",
    "    self.mat[self.dictionary[w]] = np.array(val)\n",
    "\n",
    "\n",
    "  def load(self) :\n",
    "    verbose = 1\n",
    "    if self.path.endswith('xz'):\n",
    "      openner = lzma.open\n",
    "      encoding = 'latin-1'\n",
    "      verbose = 0 # does not show the bar correctly\n",
    "    else:\n",
    "      openner = open\n",
    "      encoding = 'utf-8'\n",
    "    with openner(self.path, 'rt', encoding=encoding) as fp :\n",
    "      cnt = 0\n",
    "      progress = 0\n",
    "\n",
    "      line = fp.readline()\n",
    "      tokens = line.split(self.delim) \n",
    "      if self.dim < 0:\n",
    "        if len(tokens) == 2 :\n",
    "          self.dim = int(tokens[1])\n",
    "          line = fp.readline()\n",
    "        else :\n",
    "          self.dim = len(tokens)-1\n",
    "\n",
    "      assert(self.dim > 0)\n",
    "      \n",
    "      self.mat.resize((0,self.dim), refcheck=False)\n",
    "\n",
    "      while line :\n",
    "        if not(cnt%self.batch_sz) :\n",
    "          (r,c) = self.mat.shape\n",
    "          self.mat.resize((r+self.batch_sz,c), refcheck=False)\n",
    "\n",
    "        if verbose and not(cnt%1000):\n",
    "          progress = fp.tell()*1.0/self.size\n",
    "          #utils.update_progress(progress,\"Loading word vectors\", 40)\n",
    "\n",
    "        tokens = line.rstrip().split(self.delim)\n",
    "        assert(len(tokens) == self.dim + 1)\n",
    "        self.dictionary[tokens[0]] = cnt \n",
    "        self.mat[cnt] = np.array([float(x) for x in tokens[1:]])\n",
    "\n",
    "        line = fp.readline()\n",
    "        cnt += 1\n",
    "\n",
    "      if cnt < self.mat.shape[0] : self.mat.resize((cnt,self.dim), refcheck=False)\n",
    "\n",
    "      #utils.update_progress(1,\"Loading word vectors\", 40)\n",
    "    return  \n",
    "\n",
    "  def cosine_dist(self, w1, w2) :\n",
    "    return cosine(self.dictionary[w1],self.dictionary[w2])\n",
    "\n",
    "  def update_item(self, word, vector) :\n",
    "    assert(len(vector) == self.dim)\n",
    "    try :\n",
    "      idx = self.dictionary[word]\n",
    "    except KeyError :\n",
    "      (r,c) = self.mat.shape \n",
    "      self.dictionary[word] = r\n",
    "      self.mat.resize((r+1,c))\n",
    "      idx = r\n",
    "    self.mat[idx] = vector\n",
    "\n",
    "  def normalize(self):\n",
    "    mean = np.mean(self.mat, axis=0).tolist()\n",
    "    std = np.std(self.mat, axis=0).tolist()\n",
    "    self.mat = (self.mat - mean)/std \n",
    "\n",
    "###########################################\n",
    "class MultiWordVector:\n",
    "  def __init__(self, word_vectors):\n",
    "    self.word_vectors = word_vectors\n",
    "    self.dictionary = {}\n",
    "    self.dim = -1\n",
    "    self.unk=\"<unk_vocab>\"\n",
    "    if self.word_vectors: self.dim = self.word_vectors[0].dim\n",
    "    if self.word_vectors: self.unk = self.word_vectors[0].unk\n",
    "    for wv in self.word_vectors:\n",
    "      assert(self.dim == wv.dim)\n",
    "      for w in wv.dictionary:\n",
    "        try:\n",
    "          self.dictionary[w].append(wv)\n",
    "        except KeyError:\n",
    "          self.dictionary[w] = [wv]\n",
    "    pass\n",
    "\n",
    "  def __getitem__(self, w):\n",
    "    try:\n",
    "      return self.dictionary[w][0][w]\n",
    "    except KeyError:\n",
    "      return self.word_vectors[0][w]\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, vocabs):\n",
    "  wv = WordVector(path)\n",
    "  embedding_matrix = numpy.zeros((len(vocabs), wv.dim))\n",
    "\n",
    "  for w,idx in vocabs.items():\n",
    "    embedding_matrix[idx] = wv[w]\n",
    "  return torch.from_numpy(embedding_matrix.astype('float32'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pdb\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from self_attention import SelfAttention\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, input_dim, attention_dim, annotation_dim, dropout_rate=0.5, regulization_rate=1e-2):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.input_dim = input_dim\n",
    "    self.att_dim = attention_dim\n",
    "    self.ann_dim = annotation_dim\n",
    "    self.regulization_rate = regulization_rate\n",
    "\n",
    "    self.w1 = nn.Linear(self.input_dim, self.att_dim, bias=False)\n",
    "    self.w2 = nn.Linear(self.att_dim, self.ann_dim, bias=False)\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "  def forward(self, H): \n",
    "    #batch_size, sequence_length, feature_size = H.shape\n",
    "    x = self.w1(H) #x.shape: (batch_size, sequence_length, attention_dim)\n",
    "    x = self.dropout(x)\n",
    "    x = self.w2(x) #x.shape: (batch_size, sequence_len, annotation_dim)\n",
    "    x = self.dropout(x)\n",
    "    \n",
    "    A = F.softmax(x, dim=1) #A.shape: (batch_size, sequence_len, annotation_dim)\n",
    "    M = torch.bmm(H.transpose(1,-1),A) #M.shape=(batch_size, features_size, annotation_dim)\n",
    "\n",
    "    #AAT = torch.bmm(A,A.transpose(1,-1))\n",
    "    #P = torch.norm(AAT - torch.eye(sequence_length))\n",
    "    #P = self.regulization_rate * torch.norm(AAT - torch.eye(sequence_length))\n",
    "    return M, A\n",
    " \n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_mat, padding_idx, num_lstm_layers=2, bidirectional=True):\n",
    "    super().__init__()\n",
    "\n",
    "    embedding_dim   = embedding_mat.shape[1]    \n",
    "    #self.embedding_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx)#, _weight=embedding_mat)\n",
    "\n",
    "    self.embedding_layer = nn.Embedding.from_pretrained(embedding_mat,padding_idx=0, freeze=True)\n",
    "    self.embedding_generalizer = nn.Linear(embedding_dim,embedding_dim)\n",
    "\n",
    "    self.lstm_output_size = 2*embedding_dim\n",
    "    self.LSTM = nn.LSTM(embedding_dim, self.lstm_output_size, num_layers=num_lstm_layers, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "    #self.n_output_dim = lstm_output_size * num_lstm_layers * (1 + bidirectional)\n",
    " \n",
    "  def forward(self, x):\n",
    "    emb = self.embedding_layer(x) #Input\n",
    "    gemb = self.embedding_generalizer(emb)\n",
    "    output, (hn, cn) = self.LSTM(emb + gemb)\n",
    "    hn = torch.cat([h for h in hn], dim=-1) # (num_layers * num_directions, batch, hidden_size) -> (batch, hidden_size * num_layers * num_directions)\n",
    "    return output, hn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.classifier = nn.Linear(input_dim, output_dim)\n",
    "    self.activation = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.activation(self.classifier(x))\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "  def __init__(self, encoder, classifier, attention=None,modelpath=None):\n",
    "    super().__init__()\n",
    "\n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.sequence_batcher = SequenceBatcher(self.device)\n",
    "\n",
    "    self.encoder = encoder.to(self.device)\n",
    "    self.attention = attention.to(self.device) if attention else None\n",
    "    self.flatten = torch.nn.Flatten()\n",
    "    self.classifier = classifier.to(self.device)\n",
    "    self.modelpath = modelpath\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, sequence_length = x.shape\n",
    "    enc_output, enc_hn = self.encoder(x)\n",
    "    if self.attention:\n",
    "      m_att, a_att = self.attention(enc_output)\n",
    "      m_att = self.flatten(m_att)\n",
    "      enc = torch.cat([enc_hn, m_att], dim=-1)\n",
    "      #AAT = torch.bmm(a_att,a_att.transpose(1,-1))\n",
    "      p_att = 0 #1e-2 * torch.norm(AAT - torch.eye(sequence_length).to(self.device))\n",
    "    else:\n",
    "      enc = enc_hn\n",
    "      p_att = 0\n",
    "\n",
    "    return self.classifier(enc), p_att\n",
    "\n",
    "  def fit(self, train_generator, val_generator, n_epochs=1, lr=1e-2):\n",
    "    optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "    training_generator = torch.utils.data.DataLoader(train_generator, batch_size=64, shuffle=True, num_workers=0, collate_fn=self.sequence_batcher)\n",
    "    validation_generator = torch.utils.data.DataLoader(val_generator, batch_size=64, shuffle=True, num_workers=0, collate_fn=self.sequence_batcher)\n",
    "\n",
    "    BCELoss = nn.BCELoss()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_epoch_index = 0\n",
    "    early_stop_cnt = 5\n",
    "    for e in range(n_epochs):\n",
    "      losses = []\n",
    "      accs = []\n",
    "      self.train()\n",
    "      with tqdm(total=len(train_generator)) as pbar:\n",
    "        pbar.set_description(f'Training Epoch {e+1}')\n",
    "        for i, (bx, by) in enumerate(training_generator):\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          output, p_att_loss = self.forward(bx)\n",
    "          output = output.squeeze(-1)\n",
    "          loss = BCELoss(output, by) + p_att_loss\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          losses.append(loss.item())\n",
    "          acc = ((output > 0.5).float() == by).float().mean()\n",
    "          accs.append(acc.item())\n",
    "          pbar.set_postfix(acc=(sum(accs)/len(accs)), loss=(sum(losses) / len(losses)))\n",
    "          pbar.update(len(bx))\n",
    "\n",
    "      if val_generator and e % 1 == 0:\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        self.eval()\n",
    "        with tqdm(total=len(val_generator)) as pbar:\n",
    "          pbar.set_description(f'Validation Epoch {e+1}')\n",
    "\n",
    "          for i, (bx, by) in enumerate(validation_generator):\n",
    "            with torch.no_grad():\n",
    "              output, p_att_loss = self.forward(bx)\n",
    "            output = output.squeeze(-1)\n",
    "            loss = BCELoss(output, by) + p_att_loss\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            \n",
    "            acc = ((output > 0.5).float() == by).float().mean()\n",
    "            val_accs.append(acc.item())\n",
    "            pbar.set_postfix(acc=(sum(val_accs) / len(val_accs)), loss=(sum(val_losses) / len(val_losses)))\n",
    "            pbar.update(len(bx))\n",
    "          \n",
    "          val_acc = sum(val_accs) / len(val_accs)\n",
    "          if val_acc > 0.99:\n",
    "            torch.save(self.state_dict(), self.modelpath)\n",
    "            break\n",
    "\n",
    "          if val_acc > best_val_acc:\n",
    "            torch.save(self.state_dict(), self.modelpath)\n",
    "            best_epoch_index = e\n",
    "            best_val_acc = val_acc\n",
    "          \n",
    "          if e + 1 - best_epoch_index >= early_stop_cnt:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From training building a vocab set\n",
    "def training_language(lang,emb):\n",
    "    train_corpus_path = \"D:\\\\data\\\\raw_data\\\\\"+lang+\".raw.trn\"\n",
    "    val_corpus_path = \"D:\\\\data\\\\raw_data\\\\\"+lang+\".raw.val\"\n",
    "    vocabs_path = \"D:\\\\data\\\\raw_data\\\\\"+lang+\".raw.trn.vcb\"\n",
    "    neg_size    = 10\n",
    "    max_vocab   = 1000000\n",
    "    epochs      = 8\n",
    "    histpath    = \"D:\\\\data\\\\raw_data\\\\\"+lang+\"_raw.hist.pkl\"\n",
    "    modelpath   = \"D:\\\\data\\\\raw_data\\\\lstm_with_attention\\\\\"+lang+\"_raw.model\"\n",
    "    vocabs_dict = \"D:\\\\data\\\\raw_data\\\\\"+lang+\"_raw.vcb.pkl\"\n",
    "    unk         = '<unknwon>'\n",
    "    embpath     = \"D:\\\\data\\\\raw_data\\\\word_vectors\\\\wiki.\"+emb+\".align.vec\"\n",
    "\n",
    "\n",
    "    vfreqs = {}\n",
    "    \n",
    "    if os.path.isfile(modelpath):\n",
    "        print(\"Already trained :) \", lang)\n",
    "        return    \n",
    "\n",
    "    try:\n",
    "        with open(vocabs_path, mode=\"r\", encoding=\"utf-8\") as fp:\n",
    "            for line in fp:\n",
    "                (word,frq) = line.strip().split('\\t')\n",
    "                vfreqs[word] = int(frq)\n",
    "        corpus_size = vfreqs['<s>']\n",
    "    except IOError:\n",
    "        print(\"No input file\")\n",
    "        return\n",
    "\n",
    "    vfreqs = {}\n",
    "    with open(vocabs_path, mode=\"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            (word,frq) = line.strip().split('\\t')\n",
    "            vfreqs[word] = int(frq)\n",
    "    corpus_size = vfreqs['<s>']\n",
    "\n",
    "    vocabs = [k for k, v in sorted(vfreqs.items(), key=lambda item: item[1])]\n",
    "    max_vocabs = min([max_vocab, len(vocabs)])\n",
    "    vocabs = vocabs[::-1][0:max_vocab-1]\n",
    "    # 0 is reserved for masking\n",
    "    vocabs = {k:i for i,k in enumerate(vocabs,2)}\n",
    "    vocabs[unk] = 1\n",
    "    vocabs['<PAD>'] = 0\n",
    "    with open(vocabs_dict, 'wb') as fp:\n",
    "        pickle.dump(vocabs, fp, protocol=2)\n",
    "\n",
    "    emb_mat = load_embeddings(embpath, vocabs)\n",
    "\n",
    "    train_gen = Shuffler(train_corpus_path, vocabs, neg_size, corpus_size, unk=unk, weight=False, max_snt=0)\n",
    "    val_gen = Shuffler(val_corpus_path, vocabs, neg_size, corpus_size, unk=unk, weight=False, max_snt=0)\n",
    "\n",
    "    print(\"train size: {0}\".format(len(train_gen)))\n",
    "    print(\"validation size: {0}\".format(len(val_gen)))\n",
    "\n",
    "    bidirectional=True\n",
    "    num_lstm_layers=1\n",
    "    encoder = Encoder(len(vocabs), emb_mat, 0, bidirectional=bidirectional, num_lstm_layers=num_lstm_layers)\n",
    "\n",
    "    att_input_dim = (int(bidirectional)+1)*encoder.lstm_output_size\n",
    "    att_dim = 10\n",
    "    ann_dim = 1\n",
    "    attention = SelfAttention(att_input_dim, att_dim, ann_dim)\n",
    "\n",
    "    cls_input_dim = encoder.lstm_output_size * num_lstm_layers * (int(bidirectional)+1) + (att_input_dim*ann_dim if attention else 0)\n",
    "    classifier = Classifier(cls_input_dim, 1)\n",
    "\n",
    "    snt_classifier = SentenceClassifier(encoder, classifier, attention,modelpath)\n",
    "    snt_classifier.fit(train_gen, val_gen, lr=0.01, n_epochs=epochs)\n",
    "\n",
    "    print('saving model in ', modelpath)\n",
    "    torch.save(snt_classifier.state_dict(), modelpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from the existing pickle file D:\\data\\raw_data\\word_vectors\\wiki.sv.align.vec.{dic,mat}.pkl\n",
      "train size: 1361270\n",
      "validation size: 1358522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████████████████████████| 1361270/1361270 [21:16<00:00, 1066.64it/s, acc=0.85, loss=0.311]\n",
      "Validation Epoch 1: 100%|███████████████████████████| 1358522/1358522 [08:08<00:00, 2782.00it/s, acc=0.903, loss=0.214]\n",
      "Training Epoch 2: 100%|██████████████████████████████| 1361270/1361270 [27:06<00:00, 836.95it/s, acc=0.866, loss=0.285]\n",
      "Validation Epoch 2: 100%|███████████████████████████| 1358522/1358522 [07:32<00:00, 3004.81it/s, acc=0.874, loss=0.268]\n",
      "Training Epoch 3:  30%|█████████                     | 408384/1361270 [06:08<14:30, 1094.69it/s, acc=0.861, loss=0.294]"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import gc\n",
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import pdb\n",
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "\n",
    "#embedding_dict={\"Afrikaans\":\"af\",\"Arabic\":\"ar\",\"Bulgarian\":\"bg\",\"Bengali\":\"bn\",\"Bosnian\":\"bs\",\"Catalan\":\"ca\",\"Czech\":\"cs\",\"Danish\":\"da\",\"German\":\"de\",\"English\":\"en\",\"Spanish\":\"es\",\"Estonian\":\"et\",\"Persian\":\"fa\",\"Finnish\":\"fi\",\"French\":\"fr\",\"Hebrew\":\"he\",\"Hindi\":\"hi\",\"Croatian\":\"hr\",\"Hungarian\":\"hu\",\"Indonesian\":\"id\",\"Italian\":\"it\",\"Korean\":\"ko\",\"Latvian\":\"lv\",\"Macedonian\":\"mk\",\"Malay\":\"ms\",\"Dutch\":\"nl\",\"Polish\":\"pl\",\"Portuguese\":\"pt\",\"Romanian\":\"ro\",\"Russian\":\"ru\",\"Slovak\":\"sk\",\"Slovenian\":\"sl\",\"Albanian\":\"sq\",\"Swedish\":\"sv\",\"Thai\":\"th\",\"Tagalog\":\"tl\",\"Turkish\":\"tr\",\"Ukrainian\":\"uk\",\"Vietnamese\":\"vi\"}\n",
    "embedding_dict = {\"Swedish\":\"sv\"}\n",
    "j=0\n",
    "for i in embedding_dict.keys():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    training_language(i,embedding_dict[i])\n",
    "    j+=1\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from the existing pickle file D:\\data\\raw_data\\word_vectors\\wiki.da.align.vec.{dic,mat}.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing on native data\n",
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import pdb\n",
    "import math\n",
    "import numpy\n",
    "import torch \n",
    "\n",
    "vocabs_path=\"D:\\\\data\\\\raw_data\\\\Danish.raw.trn.vcb\"\n",
    "embpath=\"D:\\\\data\\\\raw_data\\\\word_vectors\\\\wiki.da.align.vec\"\n",
    "neg_size    = 10\n",
    "max_vocab   = 1000000\n",
    "unk         = '<unknwon>'\n",
    "vfreqs = {}\n",
    "vocabs_dict = \"D:\\\\data\\\\raw_data\\\\Danish_raw.vcb.pkl\"\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(vocabs_path, mode=\"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            (word,frq) = line.strip().split('\\t')\n",
    "            vfreqs[word] = int(frq)\n",
    "    corpus_size = vfreqs['<s>']\n",
    "except IOError:\n",
    "    print(\"No input file\")\n",
    "\n",
    "\n",
    "vocabs = [k for k, v in sorted(vfreqs.items(), key=lambda item: item[1])] \n",
    "max_vocabs = min([max_vocab, len(vocabs)])\n",
    "vocabs = vocabs[::-1][0:max_vocab-1]\n",
    "# 0 is reserved for masking\n",
    "vocabs = {k:i for i,k in enumerate(vocabs,2)}\n",
    "vocabs['<unknwon>'] = 1\n",
    "vocabs['<PAD>'] = 0\n",
    "with open(vocabs_dict, 'wb') as fp:\n",
    "    pickle.dump(vocabs, fp, protocol=2)\n",
    "\n",
    "emb_mat = load_embeddings(embpath, vocabs)  \n",
    "\n",
    "\n",
    "bidirectional=True\n",
    "num_lstm_layers=1\n",
    "encoder = Encoder(len(vocabs), emb_mat, 0, bidirectional=bidirectional, num_lstm_layers=num_lstm_layers)\n",
    "\n",
    "att_input_dim = (int(bidirectional)+1)*encoder.lstm_output_size\n",
    "att_dim = 10\n",
    "ann_dim = 1\n",
    "attention = SelfAttention(att_input_dim, att_dim, ann_dim)\n",
    "\n",
    "cls_input_dim = encoder.lstm_output_size * num_lstm_layers * (int(bidirectional)+1) + (att_input_dim*ann_dim if attention else 0)\n",
    "classifier = Classifier(cls_input_dim, 1)\n",
    "\n",
    "the_model = SentenceClassifier(encoder, classifier, attention,None)\n",
    "\n",
    "modelpath=\"D:\\\\data\\\\raw_data\\\\lstm_with_attention\\\\Danish_raw.model\"\n",
    "the_model.load_state_dict(torch.load(modelpath),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8263370978052419\n",
      "0.8385050003620536\n",
      "0.9166912217433372\n",
      "0.875856686037961\n",
      "tensor(0.3945, dtype=torch.float64)\n",
      "0.6682880592518571\n"
     ]
    }
   ],
   "source": [
    "#Testing on native data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data_path=\"D:\\\\data\\\\raw_data\\\\Danish.raw.tst\"\n",
    "data = Shuffler(data_path, vocabs, neg_size, corpus_size, unk=unk, weight=False, max_snt=0)\n",
    "data_new = torch.utils.data.DataLoader(data, batch_size=64, shuffle=True, num_workers=0, collate_fn=SequenceBatcher('cuda'))\n",
    "the_model.eval()\n",
    "BCELoss=nn.BCELoss()\n",
    "y_actual=np.array([])\n",
    "y_pred_prob=np.array([])\n",
    "for i, (bx, by) in enumerate(data_new):\n",
    "     with torch.no_grad():\n",
    "              output,p_att_loss = the_model.forward(bx)\n",
    "     output = output.squeeze(-1)\n",
    "     y_actual=np.append(y_actual,by.cpu().numpy())\n",
    "     y_pred_prob=np.append(y_pred_prob,output.cpu().numpy())\n",
    "    \n",
    "y_pred=[1 if i>0.5 else 0 for i in y_pred_prob ]\n",
    "y_pred=np.array(y_pred)\n",
    "print(accuracy_score(y_actual,y_pred))\n",
    "print(precision_score(y_actual,y_pred))\n",
    "print(recall_score(y_actual,y_pred))\n",
    "print(f1_score(y_actual,y_pred))\n",
    "print(BCELoss(torch.tensor(y_pred_prob),torch.tensor(y_actual)))\n",
    "print(sum(y_actual)/len(y_actual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from the existing pickle file D:\\data\\raw_data\\word_vectors\\wiki.hi.align.vec.{dic,mat}.pkl\n"
     ]
    }
   ],
   "source": [
    "#Cross testing\n",
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import pdb\n",
    "import math\n",
    "import numpy\n",
    "import torch \n",
    "\n",
    "vocabs_path=\"D:\\\\data\\\\raw_data\\\\Hindi.raw.trn.vcb\"\n",
    "embpath=\"D:\\\\data\\\\raw_data\\\\word_vectors\\\\wiki.hi.align.vec\"\n",
    "neg_size    = 10\n",
    "max_vocab   = 1000000\n",
    "unk         = '<unknwon>'\n",
    "vfreqs = {}\n",
    "vocabs_dict = \"D:\\\\data\\\\raw_data\\\\Hindi_raw.vcb.pkl\"\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(vocabs_path, mode=\"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            (word,frq) = line.strip().split('\\t')\n",
    "            vfreqs[word] = int(frq)\n",
    "    corpus_size = vfreqs['<s>']\n",
    "except IOError:\n",
    "    print(\"No input file\")\n",
    "\n",
    "\n",
    "vocabs = [k for k, v in sorted(vfreqs.items(), key=lambda item: item[1])] \n",
    "max_vocabs = min([max_vocab, len(vocabs)])\n",
    "vocabs = vocabs[::-1][0:max_vocab-1]\n",
    "# 0 is reserved for masking\n",
    "vocabs = {k:i for i,k in enumerate(vocabs,2)}\n",
    "vocabs['<unknwon>'] = 1\n",
    "vocabs['<PAD>'] = 0\n",
    "with open(vocabs_dict, 'wb') as fp:\n",
    "    pickle.dump(vocabs, fp, protocol=2)\n",
    "\n",
    "emb_mat = load_embeddings(embpath, vocabs)  \n",
    "\n",
    "\n",
    "bidirectional=True\n",
    "num_lstm_layers=1\n",
    "encoder = Encoder(len(vocabs), emb_mat, 0, bidirectional=bidirectional, num_lstm_layers=num_lstm_layers)\n",
    "\n",
    "att_input_dim = (int(bidirectional)+1)*encoder.lstm_output_size\n",
    "att_dim = 10\n",
    "ann_dim = 1\n",
    "attention = SelfAttention(att_input_dim, att_dim, ann_dim)\n",
    "\n",
    "cls_input_dim = encoder.lstm_output_size * num_lstm_layers * (int(bidirectional)+1) + (att_input_dim*ann_dim if attention else 0)\n",
    "classifier = Classifier(cls_input_dim, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross testing\n",
    "the_model = SentenceClassifier(encoder, classifier, attention,None)\n",
    "modelpath=\"D:\\\\data\\\\raw_data\\\\lstm_with_attention\\\\Danish_raw.model\"\n",
    "\n",
    "m1=torch.load(modelpath)\n",
    "own_state=the_model.state_dict()\n",
    "for name, param in m1.items():\n",
    "    if name=='encoder.embedding_layer.weight':\n",
    "        continue\n",
    "    else:\n",
    "        param = param.data\n",
    "        own_state[name].data.copy_(param)\n",
    "del m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6514623335212744\n",
      "0.7015306522836993\n",
      "0.8349221941662656\n",
      "0.762436039391557\n",
      "tensor(0.6945, dtype=torch.float64)\n",
      "0.6698793300662014\n"
     ]
    }
   ],
   "source": [
    "#Cross testing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data_path=\"D:\\\\data\\\\raw_data\\\\Hindi.raw.trn\"\n",
    "data = Shuffler(data_path, vocabs, neg_size, corpus_size, unk=unk, weight=False, max_snt=0)\n",
    "data_new = torch.utils.data.DataLoader(data, batch_size=64, shuffle=True, num_workers=0, collate_fn=SequenceBatcher('cuda'))\n",
    "the_model.eval()\n",
    "BCELoss=nn.BCELoss()\n",
    "y_actual=np.array([])\n",
    "y_pred_prob=np.array([])\n",
    "for i, (bx, by) in enumerate(data_new):\n",
    "     with torch.no_grad():\n",
    "              output,p_att_loss = the_model.forward(bx)\n",
    "     output = output.squeeze(-1)\n",
    "     y_actual=np.append(y_actual,by.cpu().numpy())\n",
    "     y_pred_prob=np.append(y_pred_prob,output.cpu().numpy())\n",
    "    \n",
    "y_pred=[1 if i>0.5 else 0 for i in y_pred_prob]\n",
    "y_pred=np.array(y_pred)\n",
    "print(accuracy_score(y_actual,y_pred))\n",
    "print(precision_score(y_actual,y_pred))\n",
    "print(recall_score(y_actual,y_pred))\n",
    "print(f1_score(y_actual,y_pred))\n",
    "print(BCELoss(torch.tensor(y_pred_prob),torch.tensor(y_actual)))\n",
    "print(sum(y_actual)/len(y_actual))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
