{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import pdb\n",
    "import math\n",
    "import numpy\n",
    "import torch \n",
    "\n",
    "\n",
    "class SequenceBatcher(object):\n",
    "  def __init__(self, device):\n",
    "    self.device = device\n",
    "    return\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    x, y = zip(*batch)\n",
    "    max_x_len = max(len(xx) for xx in x)\n",
    "    \n",
    "    x = torch.LongTensor([xx + [0] * (max_x_len - len(xx)) for xx in x])\n",
    "    y = torch.FloatTensor([yy for yy in y])\n",
    "\n",
    "    return x.to(self.device),y.to(self.device)\n",
    "\n",
    "\n",
    "class Shuffler(torch.utils.data.Dataset):\n",
    "  def __init__(self, corpus, vocabs, neg_size, corpus_size, max_snt=0, unk='<unknown>', neg_rate=0.1, weight=True):\n",
    "    self.corpus = corpus\n",
    "    self.neg_size = neg_size\n",
    "    self.neg_rate = neg_rate\n",
    "\n",
    "    self.weight = weight\n",
    "\n",
    "    num_grammatical_sentences = max_snt if (max_snt > 0) else corpus_size\n",
    "    self.shuffle_index = [0] + [int(random.random() < self.neg_rate) * random.randrange(self.neg_size) for _ in range(1,num_grammatical_sentences)]\n",
    "    self.temp_index = np.copy(self.shuffle_index)\n",
    "    self.num_items = sum(self.shuffle_index) + sum([int(i==0) for i in self.shuffle_index])\n",
    "\n",
    "    self.vocabs = vocabs\n",
    "    self.unk = unk\n",
    "\n",
    "    self.corpus_fp = open(corpus, mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "    self.grammatical_snt_idx = 0\n",
    "\n",
    "    self.x = [] # contains next grammatical sequence\n",
    "    self.tmp=[]\n",
    "    self.j=0\n",
    "    g=self.create_dataset()\n",
    "    self.A=g[0]\n",
    "    self.label=g[1]\n",
    "    return\n",
    "\n",
    "  def readline(self):\n",
    "    return self.corpus_fp.readline().lower().split()\n",
    "\n",
    "  def tok2id(self, tokens):\n",
    "    for ii, token in enumerate(tokens):\n",
    "      try:\n",
    "        token_id = self.vocabs[token]\n",
    "      except KeyError:\n",
    "        token_id = self.vocabs[self.unk]\n",
    "      tokens[ii] = token_id\n",
    "    return tokens\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.num_items\n",
    "\n",
    "  def create_dataset(self):\n",
    "        A=[]\n",
    "        y=[]\n",
    "        for i in range(self.num_items):\n",
    "            if not self.temp_index[self.grammatical_snt_idx]:\n",
    "                A.append(self.tok2id(self.readline()))\n",
    "                y.append(1)\n",
    "                self.grammatical_snt_idx += 1\n",
    "            else:\n",
    "                if self.temp_index[self.grammatical_snt_idx]==self.shuffle_index[self.grammatical_snt_idx]:\n",
    "                    self.tmp=self.tok2id(self.readline())\n",
    "                self.temp_index[self.grammatical_snt_idx] -= 1\n",
    "                A.append(random.sample(self.tmp[:-1],len(self.tmp[:-1])))\n",
    "                y.append(0)\n",
    "                if self.temp_index[self.grammatical_snt_idx]==0:\n",
    "                    self.grammatical_snt_idx += 1\n",
    "                \n",
    "        return [A,y]\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "        return self.A[index],self.label[index]\n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import pdb\n",
    "import math\n",
    "import numpy\n",
    "import torch \n",
    "\n",
    "\n",
    "class SequenceBatcher(object):\n",
    "  def __init__(self, device):\n",
    "    self.device = device\n",
    "    return\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    x, y = zip(*batch)\n",
    "    max_x_len = max(len(xx) for xx in x)\n",
    "    \n",
    "    x = torch.LongTensor([xx + [0] * (max_x_len - len(xx)) for xx in x])\n",
    "    y = torch.FloatTensor([yy for yy in y])\n",
    "\n",
    "    return x.to(self.device),y.to(self.device)\n",
    "\n",
    "\n",
    "class Shuffler(torch.utils.data.Dataset):\n",
    "  def __init__(self, corpus, vocabs, neg_size, corpus_size, max_snt=0, unk='<unknown>', neg_rate=0.1, weight=True):\n",
    "    self.corpus = corpus\n",
    "    self.neg_size = neg_size\n",
    "    self.neg_rate = neg_rate\n",
    "\n",
    "    self.weight = weight\n",
    "\n",
    "    num_grammatical_sentences = max_snt if (max_snt > 0) else corpus_size\n",
    "    self.shuffle_index = [0] + [int(random.random() < self.neg_rate) * random.randrange(self.neg_size) for _ in range(1,num_grammatical_sentences)]\n",
    "    self.temp_index = np.copy(self.shuffle_index)\n",
    "    self.num_items = sum(self.shuffle_index) + sum([int(i==0) for i in self.shuffle_index])\n",
    "\n",
    "    self.vocabs = vocabs\n",
    "    self.unk = unk\n",
    "\n",
    "    self.corpus_fp = open(corpus, mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "    self.grammatical_snt_idx = 0\n",
    "\n",
    "    self.x = [] # contains next grammatical sequence\n",
    "    self.tmp=[]\n",
    "    self.j=0\n",
    "    g=self.create_dataset()\n",
    "    self.A=g[0]\n",
    "    self.label=g[1]\n",
    "    return\n",
    "\n",
    "  def readline(self):\n",
    "    return self.corpus_fp.readline().lower().split()\n",
    "\n",
    "  def tok2id(self, tokens):\n",
    "    for ii, token in enumerate(tokens):\n",
    "      try:\n",
    "        token_id = self.vocabs[token]\n",
    "      except KeyError:\n",
    "        token_id = self.vocabs[self.unk]\n",
    "      tokens[ii] = token_id\n",
    "    return tokens\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.num_items\n",
    "\n",
    "  def create_dataset(self):\n",
    "        A=[]\n",
    "        y=[]\n",
    "        for i in range(self.num_items):\n",
    "            if not self.temp_index[self.grammatical_snt_idx]:\n",
    "                A.append(self.tok2id(self.readline()))\n",
    "                y.append(1)\n",
    "                self.grammatical_snt_idx += 1\n",
    "            else:\n",
    "                if self.temp_index[self.grammatical_snt_idx]==self.shuffle_index[self.grammatical_snt_idx]:\n",
    "                    self.tmp=self.tok2id(self.readline())\n",
    "                self.temp_index[self.grammatical_snt_idx] -= 1\n",
    "                A.append(random.sample(self.tmp[:-1],len(self.tmp[:-1])))\n",
    "                y.append(0)\n",
    "                if self.temp_index[self.grammatical_snt_idx]==0:\n",
    "                    self.grammatical_snt_idx += 1\n",
    "                \n",
    "        return [A,y]\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "        return self.A[index],self.label[index]\n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import utils\n",
    "import os\n",
    "import lzma # to read xz files\n",
    "\n",
    "import pickle\n",
    "\n",
    "class WordVector :\n",
    "  def __init__(self) :\n",
    "    return\n",
    "\n",
    "  def __init__(self, path, unk=\"<unk_vocab>\", beg='<s>', end='</s>', vcb_list=None) :\n",
    "    self.path = path \n",
    "    self.dim = -1\n",
    "    self.unk = unk\n",
    "    self.beg = beg\n",
    "    self.end = end\n",
    "    self.delim = \" \"\n",
    "    self.dic_pickle_path = self.path + '.dic.pkl'\n",
    "    self.mat_pickle_path = self.path + '.mat.pkl'\n",
    "    self.dictionary = {}\n",
    "    self.batch_sz = 100000\n",
    "    self.mat = np.zeros(shape=(0,0))\n",
    "\n",
    "    try :\n",
    "      with open(self.dic_pickle_path, 'rb') as dic_handle, open(self.mat_pickle_path, 'rb') as mat_handle:\n",
    "        print((\"Loading from the existing pickle file {0}\".format(path + '.{dic,mat}.pkl')))\n",
    "        self.dictionary = pickle.load(dic_handle)\n",
    "        self.mat = pickle.load(mat_handle)\n",
    "        self.dim = self.mat.shape[1]\n",
    "    except FileNotFoundError: \n",
    "      self.size = os.stat(path).st_size \n",
    "      print(\"HELOOOOO\")\n",
    "      self.load() \n",
    "\n",
    "    if not (self.unk in list(self.dictionary.keys())) : \n",
    "      print((\"Warning: could not find the unknown token {0}. A zero vector will be used instead\".format(self.unk)))\n",
    "      self.update_item(self.unk, np.array([0]*self.dim)) \n",
    "    if not (self.beg in list(self.dictionary.keys())) : \n",
    "      print((\"Warning: could not find the sentence beginning token {0}. A zero vector will be used instead\".format(self.beg)))\n",
    "      self.update_item(self.beg, np.array([0]*self.dim)) \n",
    "    if not (self.end in list(self.dictionary.keys())) : \n",
    "      print((\"Warning: could not find the sentence ending token {0}. A zero vector will be used instead\".format(self.end)))\n",
    "      self.update_item(self.end, np.array([0]*self.dim)) \n",
    "\n",
    "    self.vocabs = list(self.dictionary.keys())\n",
    "\n",
    "    if not os.path.isfile(self.dic_pickle_path):\n",
    "      with open(self.dic_pickle_path, 'wb') as handle:\n",
    "        pickle.dump(self.dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    if not os.path.isfile(self.mat_pickle_path):\n",
    "      with open(self.mat_pickle_path, 'wb') as handle:\n",
    "        pickle.dump(self.mat, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    if vcb_list is not None:\n",
    "      vcb_intersect = set(self.vocabs).intersection(set(vcb_list))\n",
    "      vcb_intersect = vcb_intersect.union(set([self.unk, self.beg, self.end]))\n",
    "      mat = np.zeros((len(vcb_intersect), self.dim))\n",
    "      dic = {}\n",
    "      for i,w in enumerate(list(vcb_intersect)):\n",
    "        dic[w] = i\n",
    "        mat[i] = self[w]\n",
    "      self.dictionary = dic\n",
    "      self.mat = mat\n",
    "      self.vocabs = list(vcb_intersect)\n",
    "\n",
    "\n",
    "\n",
    "  def __getitem__(self, w): \n",
    "    if isinstance(w,int) :\n",
    "      return self.mat[w]\n",
    "\n",
    "    try :\n",
    "      return self.mat[self.dictionary[w]]\n",
    "    except KeyError :\n",
    "      return self.mat[self.dictionary[self.unk]]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dictionary)\n",
    "\n",
    "  def __setitem__(self, w, val): \n",
    "    assert(len(val) == self.dim)\n",
    "    self.mat[self.dictionary[w]] = np.array(val)\n",
    "\n",
    "\n",
    "  def load(self) :\n",
    "    verbose = 1\n",
    "    if self.path.endswith('xz'):\n",
    "      openner = lzma.open\n",
    "      encoding = 'latin-1'\n",
    "      verbose = 0 # does not show the bar correctly\n",
    "    else:\n",
    "      openner = open\n",
    "      encoding = 'utf-8'\n",
    "    with openner(self.path, 'rt', encoding=encoding) as fp :\n",
    "      cnt = 0\n",
    "      progress = 0\n",
    "\n",
    "      line = fp.readline()\n",
    "      tokens = line.split(self.delim) \n",
    "      if self.dim < 0:\n",
    "        if len(tokens) == 2 :\n",
    "          self.dim = int(tokens[1])\n",
    "          line = fp.readline()\n",
    "        else :\n",
    "          self.dim = len(tokens)-1\n",
    "\n",
    "      assert(self.dim > 0)\n",
    "      \n",
    "      self.mat.resize((0,self.dim), refcheck=False)\n",
    "\n",
    "      while line :\n",
    "        if not(cnt%self.batch_sz) :\n",
    "          (r,c) = self.mat.shape\n",
    "          self.mat.resize((r+self.batch_sz,c), refcheck=False)\n",
    "\n",
    "        if verbose and not(cnt%1000):\n",
    "          progress = fp.tell()*1.0/self.size\n",
    "          #utils.update_progress(progress,\"Loading word vectors\", 40)\n",
    "\n",
    "        tokens = line.rstrip().split(self.delim)\n",
    "        assert(len(tokens) == self.dim + 1)\n",
    "        self.dictionary[tokens[0]] = cnt \n",
    "        self.mat[cnt] = np.array([float(x) for x in tokens[1:]])\n",
    "\n",
    "        line = fp.readline()\n",
    "        cnt += 1\n",
    "\n",
    "      if cnt < self.mat.shape[0] : self.mat.resize((cnt,self.dim), refcheck=False)\n",
    "\n",
    "      #utils.update_progress(1,\"Loading word vectors\", 40)\n",
    "    return  \n",
    "\n",
    "  def cosine_dist(self, w1, w2) :\n",
    "    return cosine(self.dictionary[w1],self.dictionary[w2])\n",
    "\n",
    "  def update_item(self, word, vector) :\n",
    "    assert(len(vector) == self.dim)\n",
    "    try :\n",
    "      idx = self.dictionary[word]\n",
    "    except KeyError :\n",
    "      (r,c) = self.mat.shape \n",
    "      self.dictionary[word] = r\n",
    "      self.mat.resize((r+1,c))\n",
    "      idx = r\n",
    "    self.mat[idx] = vector\n",
    "\n",
    "  def normalize(self):\n",
    "    mean = np.mean(self.mat, axis=0).tolist()\n",
    "    std = np.std(self.mat, axis=0).tolist()\n",
    "    self.mat = (self.mat - mean)/std \n",
    "\n",
    "###########################################\n",
    "class MultiWordVector:\n",
    "  def __init__(self, word_vectors):\n",
    "    self.word_vectors = word_vectors\n",
    "    self.dictionary = {}\n",
    "    self.dim = -1\n",
    "    self.unk=\"<unk_vocab>\"\n",
    "    if self.word_vectors: self.dim = self.word_vectors[0].dim\n",
    "    if self.word_vectors: self.unk = self.word_vectors[0].unk\n",
    "    for wv in self.word_vectors:\n",
    "      assert(self.dim == wv.dim)\n",
    "      for w in wv.dictionary:\n",
    "        try:\n",
    "          self.dictionary[w].append(wv)\n",
    "        except KeyError:\n",
    "          self.dictionary[w] = [wv]\n",
    "    pass\n",
    "\n",
    "  def __getitem__(self, w):\n",
    "    try:\n",
    "      return self.dictionary[w][0][w]\n",
    "    except KeyError:\n",
    "      return self.word_vectors[0][w]\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, vocabs):\n",
    "  wv = WordVector(path)\n",
    "  embedding_matrix = numpy.zeros((len(vocabs), wv.dim))\n",
    "\n",
    "  for w,idx in vocabs.items():\n",
    "    embedding_matrix[idx] = wv[w]\n",
    "  return torch.from_numpy(embedding_matrix.astype('float32'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pdb\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    " \n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_mat, padding_idx, num_layers=1, bidirectional=True):\n",
    "    super().__init__()\n",
    "\n",
    "    embedding_dim   = embedding_mat.shape[1]    \n",
    "    #self.embedding_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx)#, _weight=embedding_mat)\n",
    "\n",
    "    self.embedding_layer = nn.Embedding.from_pretrained(embedding_mat,padding_idx=0, freeze=True)\n",
    "    self.embedding_generalizer = nn.Linear(embedding_dim,embedding_dim)\n",
    "    \n",
    "    self.pos_encoder = PositionalEncoding(embedding_dim)\n",
    "    \n",
    "    self.encoder_layers = TransformerEncoderLayer(embedding_dim, nhead=6,batch_first=True)\n",
    "    \n",
    "    #self.output_size=2*embedding_dim\n",
    "    \n",
    "    #self.rnn = nn.RNN(embedding_dim, self.output_size,num_layers=num_layers, batch_first=True,bidirectional=bidirectional)   \n",
    "\n",
    "   \n",
    "    #self.n_output_dim = lstm_output_size * num_lstm_layers * (1 + bidirectional)\n",
    " \n",
    "  def forward(self, x):\n",
    "    emb = self.embedding_layer(x) #Input\n",
    "    gemb = self.embedding_generalizer(emb)\n",
    "    src = self.pos_encoder(emb + gemb) \n",
    "    enc_output = self.encoder_layers(src)\n",
    "    #op, hn = self.rnn(enc_output)\n",
    "    #hn = torch.cat([h for h in hn], dim=-1) # (num_layers * num_directions, batch, hidden_size) -> (batch, hidden_size * num_layers * num_directions)\n",
    "    a = torch.mean(enc_output,dim=1)\n",
    "    #return op,hn\n",
    "    return a\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.classifier = nn.Linear(input_dim, output_dim)\n",
    "    self.activation = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.activation(self.classifier(x))\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "  def __init__(self, encoder, classifier,modelpath):\n",
    "    super().__init__()\n",
    "\n",
    "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #self.device='cpu'\n",
    "    self.sequence_batcher = SequenceBatcher(self.device)\n",
    "    self.modelpath = modelpath\n",
    "\n",
    "    self.encoder = encoder.to(self.device)\n",
    "    self.flatten = torch.nn.Flatten()\n",
    "    self.classifier = classifier.to(self.device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, sequence_length = x.shape\n",
    "    #enc_output, enc_hn = self.encoder(x)\n",
    "    #enc = enc_hn\n",
    "    enc = self.encoder(x)\n",
    "\n",
    "    return self.classifier(enc)\n",
    "\n",
    "  def fit(self, train_generator, val_generator, n_epochs=1, lr=1e-2):\n",
    "    optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "    training_generator = torch.utils.data.DataLoader(train_generator, batch_size=64, shuffle=True, num_workers=0, collate_fn=self.sequence_batcher)\n",
    "    validation_generator = torch.utils.data.DataLoader(val_generator, batch_size=64, shuffle=True, num_workers=0, collate_fn=self.sequence_batcher)\n",
    "\n",
    "    BCELoss = nn.BCELoss()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_epoch_index = 0\n",
    "    early_stop_cnt = 5\n",
    "    for e in range(n_epochs):\n",
    "      losses = []\n",
    "      accs = []\n",
    "      self.train()\n",
    "      with tqdm(total=len(train_generator)) as pbar:\n",
    "        pbar.set_description(f'Training Epoch {e+1}')\n",
    "        for i, (bx, by) in enumerate(training_generator):\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          output = self.forward(bx)\n",
    "          output = output.squeeze(-1)\n",
    "          loss = BCELoss(output, by)\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          losses.append(loss.item())\n",
    "          acc = ((output > 0.5).float() == by).float().mean()\n",
    "          accs.append(acc.item())\n",
    "          pbar.set_postfix(acc=(sum(accs)/len(accs)), loss=(sum(losses) / len(losses)))\n",
    "          pbar.update(len(bx))\n",
    "\n",
    "      if val_generator and e % 1 == 0:\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        self.eval()\n",
    "        with tqdm(total=len(val_generator)) as pbar:\n",
    "          pbar.set_description(f'Validation Epoch {e+1}')\n",
    "\n",
    "          for i, (bx, by) in enumerate(validation_generator):\n",
    "            with torch.no_grad():\n",
    "              output = self.forward(bx)\n",
    "            output = output.squeeze(-1)\n",
    "            loss = BCELoss(output, by) \n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            \n",
    "            acc = ((output > 0.5).float() == by).float().mean()\n",
    "            val_accs.append(acc.item())\n",
    "            pbar.set_postfix(acc=(sum(val_accs) / len(val_accs)), loss=(sum(val_losses) / len(val_losses)))\n",
    "            pbar.update(len(bx))\n",
    "          \n",
    "          val_acc = sum(val_accs) / len(val_accs)\n",
    "          if val_acc > 0.99:\n",
    "            torch.save(self.state_dict(), self.modelpath)\n",
    "            break\n",
    "\n",
    "          if val_acc > best_val_acc:\n",
    "            torch.save(self.state_dict(), self.modelpath)\n",
    "            best_epoch_index = e\n",
    "            best_val_acc = val_acc\n",
    "          \n",
    "          if e + 1 - best_epoch_index >= early_stop_cnt:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From training building a vocab set\n",
    "def training_language(lang,emb):\n",
    "    train_corpus_path = \"D:\\\\data\\\\raw_data\\\\\"+lang+\".raw.trn\"\n",
    "    val_corpus_path = \"D:\\\\data\\\\raw_data\\\\\"+lang+\".raw.val\"\n",
    "    vocabs_path = \"D:\\\\data\\\\raw_data\\\\\"+lang+\".raw.trn.vcb\"\n",
    "    neg_size    = 10\n",
    "    max_vocab   = 1000000\n",
    "    epochs      = 8\n",
    "    histpath    = \"D:\\\\data\\\\raw_data\\\\\"+lang+\"_raw.hist.pkl\"\n",
    "    modelpath   = \"D:\\\\data\\\\raw_data\\\\transformer_model\\\\\"+lang+\"_raw.model\"\n",
    "    vocabs_dict = \"D:\\\\data\\\\raw_data\\\\\"+lang+\"_raw.vcb.pkl\"\n",
    "    unk         = '<unknwon>'\n",
    "    embpath     = \"D:\\\\data\\\\raw_data\\\\word_vectors\\\\wiki.\"+emb+\".align.vec\"\n",
    "\n",
    "\n",
    "    vfreqs = {}\n",
    "    if os.path.isfile(modelpath):\n",
    "        print(\"Already trained :) \", lang)\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(vocabs_path, mode=\"r\", encoding=\"utf-8\") as fp:\n",
    "            for line in fp:\n",
    "                (word,frq) = line.strip().split('\\t')\n",
    "                vfreqs[word] = int(frq)\n",
    "        corpus_size = vfreqs['<s>']\n",
    "    except IOError:\n",
    "        print(\"No input file\")\n",
    "        return\n",
    "    \n",
    "    vocabs = [k for k, v in sorted(vfreqs.items(), key=lambda item: item[1])] \n",
    "    max_vocabs = min([max_vocab, len(vocabs)])\n",
    "    vocabs = vocabs[::-1][0:max_vocab-1]\n",
    "    # 0 is reserved for masking\n",
    "    vocabs = {k:i for i,k in enumerate(vocabs,2)}\n",
    "    vocabs['<unknwon>'] = 1\n",
    "    vocabs['<PAD>'] = 0\n",
    "    with open(vocabs_dict, 'wb') as fp:\n",
    "        pickle.dump(vocabs, fp, protocol=2)\n",
    "\n",
    "    emb_mat = load_embeddings(embpath, vocabs)  \n",
    "    train_gen = Shuffler(train_corpus_path, vocabs, neg_size, corpus_size, unk=unk, weight=False, max_snt=0)\n",
    "    val_gen = Shuffler(val_corpus_path, vocabs, neg_size, corpus_size, unk=unk, weight=False, max_snt=0)\n",
    "\n",
    "    \n",
    "    print(\"train size for language {0}: {1}\".format(lang,len(train_gen)))\n",
    "    print(\"validation size for language {0}: {1}\".format(lang,len(val_gen)))\n",
    "\n",
    "\n",
    "    bidirectional=True\n",
    "    num_layers=1\n",
    "    encoder = Encoder(len(vocabs), emb_mat, 0, bidirectional=bidirectional, num_layers=num_layers)\n",
    "\n",
    "    #cls_input_dim = encoder.output_size * 1 * (int(bidirectional)+1)\n",
    "    cls_input_dim = emb_mat.shape[1]\n",
    "    classifier = Classifier(cls_input_dim, 1)\n",
    "\n",
    "    snt_classifier = SentenceClassifier(encoder, classifier,modelpath)\n",
    "    print(\"Building model for the language: \",lang, \"\\n\")\n",
    "\n",
    "    snt_classifier.fit(train_gen, val_gen, lr=0.01, n_epochs=epochs)\n",
    "\n",
    "    print('saving model in ', modelpath)\n",
    "    torch.save(snt_classifier.state_dict(), modelpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import gc\n",
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import pdb\n",
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "\n",
    "#embedding_dict={\"Afrikaans\":\"af\",\"Arabic\":\"ar\",\"Bulgarian\":\"bg\",\"Bengali\":\"bn\",\"Bosnian\":\"bs\",\"Catalan\":\"ca\",\"Czech\":\"cs\",\"Danish\":\"da\",\"German\":\"de\",\"English\":\"en\",\"Spanish\":\"es\",\"Estonian\":\"et\",\"Persian\":\"fa\",\"Finnish\":\"fi\",\"French\":\"fr\",\"Hebrew\":\"he\",\"Hindi\":\"hi\",\"Croatian\":\"hr\",\"Hungarian\":\"hu\",\"Indonesian\":\"id\",\"Italian\":\"it\",\"Korean\":\"ko\",\"Latvian\":\"lv\",\"Macedonian\":\"mk\",\"Malay\":\"ms\",\"Dutch\":\"nl\",\"Polish\":\"pl\",\"Portuguese\":\"pt\",\"Romanian\":\"ro\",\"Russian\":\"ru\",\"Slovak\":\"sk\",\"Slovenian\":\"sl\",\"Albanian\":\"sq\",\"Swedish\":\"sv\",\"Thai\":\"th\",\"Tagalog\":\"tl\",\"Turkish\":\"tr\",\"Ukrainian\":\"uk\",\"Vietnamese\":\"vi\"}\n",
    "embedding_dict = {\"Danish\":\"da\"}\n",
    "j=0\n",
    "for i in embedding_dict.keys():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    training_language(i,embedding_dict[i])\n",
    "    j+=1\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from the existing pickle file D:\\data\\raw_data\\word_vectors\\wiki.da.align.vec.{dic,mat}.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Testing on native language dataset\n",
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import pdb\n",
    "import math\n",
    "import numpy\n",
    "import torch \n",
    "\n",
    "vocabs_path=\"D:\\\\data\\\\raw_data\\\\Danish.raw.trn.vcb\"\n",
    "embpath=\"D:\\\\data\\\\raw_data\\\\word_vectors\\\\wiki.da.align.vec\"\n",
    "neg_size    = 10\n",
    "max_vocab   = 1000000\n",
    "unk         = '<unknwon>'\n",
    "vfreqs = {}\n",
    "vocabs_dict = \"D:\\\\data\\\\raw_data\\\\Danish_raw.vcb.pkl\"\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(vocabs_path, mode=\"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            (word,frq) = line.strip().split('\\t')\n",
    "            vfreqs[word] = int(frq)\n",
    "    corpus_size = vfreqs['<s>']\n",
    "except IOError:\n",
    "    print(\"No input file\")\n",
    "\n",
    "\n",
    "vocabs = [k for k, v in sorted(vfreqs.items(), key=lambda item: item[1])] \n",
    "max_vocabs = min([max_vocab, len(vocabs)])\n",
    "vocabs = vocabs[::-1][0:max_vocab-1]\n",
    "# 0 is reserved for masking\n",
    "vocabs = {k:i for i,k in enumerate(vocabs,2)}\n",
    "vocabs['<unknwon>'] = 1\n",
    "vocabs['<PAD>'] = 0\n",
    "with open(vocabs_dict, 'wb') as fp:\n",
    "    pickle.dump(vocabs, fp, protocol=2)\n",
    "\n",
    "emb_mat = load_embeddings(embpath, vocabs)  \n",
    "\n",
    "\n",
    "bidirectional=True\n",
    "num_layers=1\n",
    "encoder = Encoder(len(vocabs), emb_mat, 0, bidirectional=bidirectional, num_layers=num_layers)\n",
    "\n",
    "#cls_input_dim = encoder.output_size * 1 * (int(bidirectional)+1)\n",
    "cls_input_dim = emb_mat.shape[1]\n",
    "classifier = Classifier(cls_input_dim, 1)\n",
    "\n",
    "the_model = SentenceClassifier(encoder, classifier,None)\n",
    "modelpath=\"D:\\\\data\\\\raw_data\\\\transformer_model\\\\Danish_raw.model\"\n",
    "the_model.load_state_dict(torch.load(modelpath),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7539351979450318\n",
      "0.7762261806091117\n",
      "0.8881356714453057\n",
      "0.8284186029110674\n",
      "tensor(0.4726, dtype=torch.float64)\n",
      "0.668836289738756\n"
     ]
    }
   ],
   "source": [
    "#Testing on native dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data_path=\"D:\\\\data\\\\raw_data\\\\Danish.raw.tst\"\n",
    "data = Shuffler(data_path, vocabs, neg_size, corpus_size, unk=unk, weight=False, max_snt=0)\n",
    "data_new = torch.utils.data.DataLoader(data, batch_size=64, shuffle=True, num_workers=0, collate_fn=SequenceBatcher('cuda'))\n",
    "the_model.eval()\n",
    "BCELoss=nn.BCELoss()\n",
    "y_actual=np.array([])\n",
    "y_pred_prob=np.array([])\n",
    "\n",
    "for i, (bx, by) in enumerate(data_new):\n",
    "     with torch.no_grad():\n",
    "              output = the_model.forward(bx)\n",
    "     output = output.squeeze(-1)\n",
    "     y_actual=np.append(y_actual,by.cpu().numpy())  \n",
    "     y_pred_prob=np.append(y_pred_prob,output.cpu().numpy())\n",
    "    \n",
    "y_pred=[1 if i>0.5 else 0 for i in y_pred_prob ]\n",
    "y_pred=np.array(y_pred)\n",
    "print(accuracy_score(y_actual,y_pred))\n",
    "print(precision_score(y_actual,y_pred))\n",
    "print(recall_score(y_actual,y_pred))\n",
    "print(f1_score(y_actual,y_pred))\n",
    "print(BCELoss(torch.tensor(y_pred_prob),torch.tensor(y_actual)))\n",
    "print(sum(y_actual)/len(y_actual))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from the existing pickle file D:\\data\\raw_data\\word_vectors\\wiki.hi.align.vec.{dic,mat}.pkl\n"
     ]
    }
   ],
   "source": [
    "#Cross learning\n",
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import pdb\n",
    "import math\n",
    "import numpy\n",
    "import torch \n",
    "\n",
    "vocabs_path=\"D:\\\\data\\\\raw_data\\\\Hindi.raw.trn.vcb\"\n",
    "embpath=\"D:\\\\data\\\\raw_data\\\\word_vectors\\\\wiki.hi.align.vec\"\n",
    "neg_size    = 10\n",
    "max_vocab   = 1000000\n",
    "unk         = '<unknwon>'\n",
    "vfreqs = {}\n",
    "vocabs_dict = \"D:\\\\data\\\\raw_data\\\\Hindi_raw.vcb.pkl\"\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(vocabs_path, mode=\"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            (word,frq) = line.strip().split('\\t')\n",
    "            vfreqs[word] = int(frq)\n",
    "    corpus_size = vfreqs['<s>']\n",
    "except IOError:\n",
    "    print(\"No input file\")\n",
    "\n",
    "\n",
    "vocabs = [k for k, v in sorted(vfreqs.items(), key=lambda item: item[1])] \n",
    "max_vocabs = min([max_vocab, len(vocabs)])\n",
    "vocabs = vocabs[::-1][0:max_vocab-1]\n",
    "# 0 is reserved for masking\n",
    "vocabs = {k:i for i,k in enumerate(vocabs,2)}\n",
    "vocabs['<unknwon>'] = 1\n",
    "vocabs['<PAD>'] = 0\n",
    "with open(vocabs_dict, 'wb') as fp:\n",
    "    pickle.dump(vocabs, fp, protocol=2)\n",
    "\n",
    "emb_mat = load_embeddings(embpath, vocabs)  \n",
    "\n",
    "\n",
    "bidirectional=True\n",
    "num_layers=1\n",
    "encoder = Encoder(len(vocabs), emb_mat, 0, bidirectional=bidirectional, num_layers=num_layers)\n",
    "\n",
    "#cls_input_dim = encoder.output_size * 1 * (int(bidirectional)+1)\n",
    "cls_input_dim = emb_mat.shape[1]\n",
    "classifier = Classifier(cls_input_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross learning\n",
    "the_model = SentenceClassifier(encoder, classifier,None)\n",
    "modelpath=\"D:\\\\data\\\\raw_data\\\\transformer_model\\\\Danish_raw.model\"\n",
    "m1=torch.load(modelpath)\n",
    "own_state=the_model.state_dict()\n",
    "for name, param in m1.items():\n",
    "    if name=='encoder.embedding_layer.weight':\n",
    "        continue\n",
    "    else:\n",
    "        param = param.data\n",
    "        own_state[name].data.copy_(param)\n",
    "del m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6016007487848828\n",
      "0.6708666410311398\n",
      "0.797305640511847\n",
      "0.7286416773421469\n",
      "tensor(0.6670, dtype=torch.float64)\n",
      "0.6708640874368489\n"
     ]
    }
   ],
   "source": [
    "#Cross learning\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data_path=\"D:\\\\data\\\\raw_data\\\\Hindi.raw.trn\"\n",
    "data = Shuffler(data_path, vocabs, neg_size, corpus_size, unk=unk, weight=False, max_snt=0)\n",
    "data_new = torch.utils.data.DataLoader(data, batch_size=64, shuffle=True, num_workers=0, collate_fn=SequenceBatcher('cuda'))\n",
    "the_model.eval()\n",
    "BCELoss=nn.BCELoss()\n",
    "y_actual=np.array([])\n",
    "y_pred_prob=np.array([])\n",
    "for i, (bx, by) in enumerate(data_new):\n",
    "     with torch.no_grad():\n",
    "              output = the_model.forward(bx)\n",
    "     output = output.squeeze(-1)\n",
    "     y_actual=np.append(y_actual,by.cpu().numpy())\n",
    "     y_pred_prob=np.append(y_pred_prob,output.cpu().numpy())\n",
    "    \n",
    "y_pred=[1 if i>0.5 else 0 for i in y_pred_prob ]\n",
    "y_pred=np.array(y_pred)\n",
    "print(accuracy_score(y_actual,y_pred))\n",
    "print(precision_score(y_actual,y_pred))\n",
    "print(recall_score(y_actual,y_pred))\n",
    "print(f1_score(y_actual,y_pred))\n",
    "print(BCELoss(torch.tensor(y_pred_prob),torch.tensor(y_actual)))\n",
    "print(sum(y_actual)/len(y_actual))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
